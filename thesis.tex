\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\usepackage{amsthm}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{mathrsfs}
\usepackage{titling}
\usepackage{hyperref}
\usepackage{url}
\usepackage{bbm}
\usepackage{xcolor}
\usepackage{graphicx}
\graphicspath{ {./Slike/} }
\usepackage{subcaption}
\usepackage{tikz-cd}

\usepackage{geometry}
\geometry{
 a4paper,
 %total={170mm,257mm},
 left=30mm,
 right=30mm,
 top=20mm,
 bottom=20mm
}

\newcommand{\A}{\mathcal{A}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\BB}{\mathscr{B}}
\newcommand{\BBB}{\mathbb{B}}
\newcommand{\D}{\mathcal{D}}
\renewcommand{\d}{\mathrm{d}}
\newcommand{\G}{\mathcal{G}}
\renewcommand{\H}{\mathcal{H}}
\newcommand{\Loc}{\mathcal{L}}
\renewcommand{\L}{\mathbb{L}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\N}{\mathbb{N}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\PP}{\mathsf{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\RR}{\mathcal{R}}
\renewcommand{\r}{\mathrm{r}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\ZZ}{\mathcal{Z}}

\newcommand{\BM}{\mathrm{BM}}
\newcommand{\Law}{\mathrm{Law}}
\newcommand{\Potts}{\mathrm{Potts}}
\newcommand{\TV}{\mathrm{TV}}
\newcommand{\extr}{\mathrm{extr}}

\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\oklepaj}[1]{\left(#1\right)}
\newcommand{\oglati}[1]{\left[#1\right]}
\newcommand{\ra}{\rightarrow}
\newcommand{\pika}{\boldsymbol{\cdot}}
\newcommand{\1}{\mathbbm{1}}
\renewcommand{\sp}[1]{\langle #1\rangle}
\newcommand{\ind}{\perp\!\!\!\!\perp}
\renewcommand{\c}{\mathsf{c}}
\newcommand{\supp}{\mathrm{supp}}
\newcommand{\5}{\vspace{0.5cm}}
\renewcommand{\tilde}{\widetilde}
\renewcommand{\hat}{\widehat}

\theoremstyle{definition}
\newtheorem{thm}{Theorem}[section]
\newtheorem{ex}[thm]{Example}
\newtheorem{prop}[thm]{Proposition}
\newtheorem*{sol}{Solution}
\newtheorem*{dis}{Disclaimer}
\newtheorem{df}[thm]{Definition}
\newtheorem{rem}[thm]{Remark}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}

\setlength{\droptitle}{-2cm}
\title{\textsc{Random Polymers Near a Homogeneous Interface}\\\vspace{0.3cm}\small{Statistical Mechanics}\vspace{-0.7cm}}
\author{Oskar Vavtar}
\date{\today}

\begin{document}
\begin{center}
\Huge{\textcolor{teal}{\texttt{TITLE PAGE}}} \\
\vspace{2cm}
\Huge{\textcolor{teal}{\texttt{TITLE PAGE}}} \\
\vspace{2cm}
Leiden University \\
\vspace{2cm}
{\textsc{Absence of Phase Transitions and Preservation of Gibbs Property under Renormalization}} \\
\vspace{2cm}
Oskar Vavtar \\
\vspace{2cm}
\Huge{\textcolor{teal}{\texttt{TITLE PAGE}}} \\
\vspace{2cm}
\Huge{\textcolor{teal}{\texttt{TITLE PAGE}}} \\
\vspace{2cm}
\Huge{\textcolor{teal}{\texttt{TITLE PAGE}}}
\end{center}
\pagebreak
\tableofcontents
\pagebreak

% #################################################################################################
% #################################################################################################
% #################################################################################################

\section{Introduction}

\textcolor{blue}{Blahblahblah}

% #################################################################################################

\subsection{Preliminaries: interactions, Hamiltonians and Gibbs measures}

The central aim of statistical mechanics is to provide a probabilistic description of microscopic physical phenomena. In particular one aims to study a collection of particles (often infinite), indexed in some lattice $\L$ and each taking values in some set $\A_x$, $x\in\L$, called the \textit{(single-)spin space}, which we will assume to be finite.\footnote{In general, one can take $\A$ to be infinite, even uncountable (e.g., $\R^d$).} Each manifestation of this macroscopic system corresponds to a (random) configuration in $\Omega=\prod_{x\in\L}\A_x$; often we will assume that the system is homogeneous, i.e., $\Omega=\A^{\L}$. In order to talk about randomness, we have to consider probability distributions on $\Omega$; we will write $\M_1(\Omega)$ for the set of probability measures on $\Omega$. One often calls those measures \textit{random fields}. \\

There is however a particular class of probability measures that are of interest, called \textit{Gibbs measures}, which we will start introducing formally in a moment, but not before giving some intuitive but \textit{very} informal description. Generally, we assume that the particles interact in some sense, which contributes some (positive or negative) energy. If some configuration carries energy $\H(\omega)$, then the Gibbs measure (consistent with $\H$) would assign $\omega$ a probability proportional to $e^{-\H(\omega)}$. Intuitively, Gibbs measures award configurations with low energy and penalize configurations with high energy. 

% *************************************************************************************************

\subsubsection{Formal definition, part 1: finite volume setting}

In this section we assume $|\L|<\infty$; an often used example would be a finite graph $G=(V,E)$. In order to capture the interactions, one defines a collection of maps $\Phi=\set{\Phi_\Lambda:\Lambda\subseteq\L}$, called \textit{interaction}, where for $\omega\in\Omega$, $\Phi_\Lambda(\omega)$, depends on $\omega$ only through its values in $\Lambda$, that is, $\Phi_\Lambda(\omega)=\Phi_\Lambda(\omega(x):x\in\Lambda)$. The value $\Phi_\Lambda(\omega)$ represents the amount of energy that the interaction between particles in $\Lambda$ contributes to the system.

\begin{ex}
Often studied are the \textit{nearest-neighbour} interactions. If $\L$ is a (not necessarily finite) graph $G=(V,E)$, then $\Phi_\Lambda$ is constant zero, unless $\Lambda=\set{x,y}$, where $x\sim y$. 
\end{ex}

The total energy of the systems is then obtained by summing up energies of all subsets:
$$\H(\omega) ~=~ \sum_{\Lambda\subseteq \L}\Phi_\Lambda(\omega), \quad \omega\in\Omega.$$
We call the function $\H=\H^\Phi$ a \textit{Hamiltonian} consistent with $\Phi$. As already suggested above, the Gibbs measure $\mu=\mu^\Phi$ is then defined so that
$$\mu(\omega) ~=~ \frac{1}{\ZZ}e^{-\H(\omega)}, \quad \omega\in\Omega,$$
where $\ZZ=\ZZ^\Phi$ is called the \textit{partition function} and is given by $\ZZ ~=~ \sum_{\omega\in\Omega}e^{-\H(\omega)}.$ In this finite volume setting, Gibbs measures also often called \textit{Gibbs canonical ensembles.} \\

While from probabilistic point of view, this may be interesting enough model, it is rather disinteresting from the statistical mechanical point of view. In particular, there is no notion of phase transitions, i.e., coexistence of several Gibbs measure consistent with the same interaction, which is one of the main focuses of study in statistical mechanics. This does, however, change when we relax the assumption about finiteness of $\L$ and take it to be a countably infinite lattice, as we will see shortly.

% *************************************************************************************************

\subsubsection{Formal definition, part 2: infinite volume setting}

In practice, one often wishes to consider $\L$ to be an infinite lattice. Unfortunately, the construction presented in the previous section doesn't translate to this setting, as the Hamiltonian \textcolor{blue}{would correspond to a infinite sum and hence the Gibbs measure wouldn't be well defined.} We thus have to consider an alternative construction of Gibbs measures in infinite volumes. The most intuitive approach perhaps appears to be to construct them as (weak) limits of measures from the previous section, as finite volumes increase to $\L$ in a suitable sense. This indeed is a valid idea: one can construct infinite-volume Gibbs measures in this way and those measures form an import class. However, it is not the entire story. We will now proceed to define (repeating some already mentioned notions) a more general notion of infinite-volume Gibbs measures, which includes all such limit measures as well as other ones. \\

From now on we restrict ourselves to $\L=\Z^d$. In the rest of the thesis, we will only consider $\L$ to be either $\Z^d$ or some subset of it. Moreover, given $\omega\in\Omega$ and $\Lambda\subseteq\Z^d$, we write $\omega_\Lambda:=\omega|_\Lambda$ as well as $\Omega_\Lambda:=\prod_{x\in\Lambda}\A_x$, so that $\omega_\Lambda\in\Omega_\Lambda$, and $\B_\Lambda$ for the (corresponding) $\sigma$-algebra on $\Omega_\Lambda$, generated by the cylinders on subsets of $\Lambda$.\footnote{The advantage of directly selecting $\omega_\Lambda$ from $\Omega_\Lambda$, rather than $\omega$ from $\Omega$ and just restricting it, is that for $\Lambda\Subset\Z^d$ one has $|\Omega_\Lambda|<\infty$, which allows for finite sums.} Also, given another $\tilde{\omega}\in\Omega$, we write $$[\omega_\Lambda\tilde{\omega}_\Lambda](x) ~:=~ \begin{cases}
\omega(x): ~&x\in\Lambda,\\
\tilde{\omega}(x): ~&x\in\Lambda^\c.
\end{cases}$$

\begin{df}~
\begin{itemize}
	\item[(1)] An \textit{interaction} is a collection of maps $\Phi=\{\Phi_\Lambda:\Lambda\Subset\Z^d\}$, such that
$$\Phi_\Lambda(\omega) ~=~ \Phi_\Lambda(\omega_\Lambda), \quad \omega\in\Omega,$$
that is, $\Phi_\Lambda$ depends on $\omega$ only through its values in $\Lambda$. We say that $\Phi$ is \textit{uniformly absolutely convergent} (write $\Phi\in\BB^1(\Omega)$) if 
$$\|\Phi\| ~:=~ \sup_{x\in\Z^d}\sum_{\substack{\Lambda\Subset\Z^d:\\x\in\Z^d}}\|\Phi_\Lambda\|_\infty ~<~ \infty$$
and write $\BB^1(\Omega)$ for the collection of all UAC interactions on $\Omega$.
	\item[(2)] For $\Phi\in\BB^1(\Omega)$, we define a corresponding collection of Hamiltonians $\H^\Phi=\set{\H_\Lambda^\Phi:\Lambda\Subset\Z^d}$, given by
	$$\H_\Lambda^\Phi(\omega) ~=~ \sum_{\substack{\Delta\Subset\Z^d:\\\Delta\cap\Lambda\neq\emptyset}}\Phi_\Lambda(\omega), \quad \omega\in\Omega.$$
	\textcolor{blue}{Add definition of quasilocal Hamiltonian?}
	\item[(3)] For $\Phi\in\BB^1(\Omega)$, we define a \textit{specification} as a collection $\gamma^\Phi=\set{\gamma_\Lambda^\Phi:\Lambda\Subset\Z^d}$ via
	$$\gamma_\Lambda^\Phi(\tilde{\omega}_\Lambda|\omega_{\Lambda^\c}) ~=~ \frac{1}{\ZZ_\Lambda^{\Phi,\omega}}\exp(-\H_\Lambda^\Phi(\tilde{\omega}_\Lambda\omega_{\Lambda^\c})), \quad \omega,\tilde{\omega}\in\Omega,$$
	where $\ZZ_\Lambda^{\Phi,\omega}=\sum_{\tilde{\omega}_\Lambda\in\Omega_\Lambda}\exp(-\H_\Lambda^\Phi(\tilde{\omega}_\Lambda\omega_{\Lambda^\c}))$.
\end{itemize}
\end{df}

\begin{rem}
Formally, $\gamma^\Phi$ should be seen as a collection of probability kernels, where for each $\Lambda\Subset\Z^d$, $\gamma_\Lambda^\Phi:\B_\Lambda\times\Omega_{\Lambda^\c}\ra(0,1)$, so $\gamma_\Lambda^\Phi(\tilde{\omega}_\Lambda|\omega_{\Lambda^\c})$ should be actually read as $\gamma_\Lambda^{\Phi}(\set{\tilde{\omega}_\Lambda}|\omega_{\Lambda^\c})$. The probability of a general $A\in\B_\Lambda$ is now given by
$$\gamma_\Lambda^\Phi(A|\omega_{\Lambda^\c}) ~=~ \sum_{\tilde{\omega}_\Lambda\in A}\gamma_\Lambda^\Phi(\tilde{\omega}_\Lambda|\omega_{\Lambda^\c}).$$
The corresponding expectation is given for suitable $f:\Omega\ra\R$ by
$$\gamma_\Lambda^\Phi[f|\omega_{\Lambda^\c}] ~=~ \sum_{\tilde{\omega}_\Lambda\in\Omega_\Lambda}f(\tilde{\omega}_\Lambda\omega_{\Lambda^\c})\gamma_{\Lambda}^\Phi(\tilde{\omega}_\Lambda|\omega_{\Lambda^\c}).$$
In further generality, we may call \textit{a specification} any collection of probability kernels $\gamma=\set{\gamma_\Lambda:\Lambda\Subset\Z^d}$ such that for each $\Lambda\Subset\Z^d$
\begin{itemize}
	\item and  $A\in\B_{\Lambda}$, $\omega\mapsto\gamma(A|\omega)$ is $\B_{\Lambda^\c}$-measurable,
	\item $\gamma_\Lambda(A|\pika)=\1_A$ for each \textcolor{purple}{$A\in\B_\Lambda$},
	\item and $\Delta\subset\Lambda$, $\int \gamma_\Delta(A|\omega')\,\gamma_\Lambda(\d\omega'|\omega)=\gamma_\Lambda(A|\omega)$.
\end{itemize}
\end{rem}

Finally, after introducing all this notions, we are able to formally define the infinite-volume Gibbs measures, using a very natural definition.

\begin{df}
A probability measure $\mu\in\M_1(\Omega)$ is said to be a Gibbs measure consistent with $\Phi\in\BB^1(\Omega)$, if it is consistent with $\gamma^\Phi$, that is, if for each $\Lambda\Subset\Z^d$ we have
$$\mu(\omega_\Lambda|\omega_{\Lambda^\c}) ~=~ \gamma_\Lambda^\Phi(\omega_\Lambda|\omega_{\Lambda^\c}) \quad~\text{for}~\mu\text{-a.a.}~\omega\in\Omega.\footnote{The notation $\mu(a_\Lambda|b_{\Lambda^\c})$ should be read as $\mu(a~\text{on}~\Lambda|b~\text{off}~\Lambda)$.}$$
We write $\G_\Omega(\Phi)$ for the set of all Gibbs measure on $\Omega$ consistent with $\Phi$. Alternatively, if our main reference is a specification $\gamma$ or a collection of Hamiltonians $\H$, we may also write $\G_\Omega(\gamma)$ or $\G_\Omega(\H)$, so in our notation $\G_\Omega(\Phi)$, $\G_\Omega(\H^\Phi)$ and $\G_\Omega(\gamma^\Phi)$ would refer to the same thing.
\end{df}

\begin{rem}
~
\begin{itemize}
	\item[(i)] Equivalently, one could define $\mu$ to be Gibbs for $\gamma$ if for each $\Lambda\Subset\Z^d$ and $f\in C(\Omega,\R)$ one has
	$$\int f\,\d\mu ~=~ \int\gamma_\Lambda[f|\omega_{\Lambda^\c}]\,\d\mu(\omega).$$
	\item[(ii)] For each $\Phi\in\BB^1(\Omega)$, $\G_\Omega(\Phi)\neq\emptyset$.
\end{itemize}
\end{rem}

\noindent\textcolor{blue}{Say something about infinite volume limits?}

\textcolor{blue}{\begin{ex}[Ising model]~\end{ex}}

Very important is the following characterization of Gibbs measures:
\begin{prop}
~
\begin{itemize}
	\item[(i)] Let $\Phi\in\BB^1(\Omega)$. Then $\gamma^\Phi$ has the following two properties:
		\begin{itemize}
			\item \textit{uniform non-nullness}: for every $\Lambda\Subset\Z^d$ there exist $\alpha_\Lambda,\beta_\Lambda\in(0,1)$ such that
			$$\alpha_\Lambda ~\leq~ \gamma_\Lambda^\Phi(\tilde{\omega}_\Lambda|\omega_{\Lambda^\c}) ~\leq~ \beta_\Lambda, \quad\forall\tilde{\omega}_\Lambda\in\Omega_\Lambda,\,\omega\in\Omega.$$
			\item \textit{quasilocality}: writing $\BBB_n=[-n,n]^d\cap\Z^d$ for $n\in\N$, for every $\Lambda\Subset\Z^d$ one has
			$$\sup_{\omega,\tilde{\omega}\in\Omega}\sup_{\xi,\zeta\in\Omega}|\gamma_\Lambda^\Phi(\tilde{\omega}_\Lambda|\omega_{\BBB_n\setminus\Lambda}\xi_{\BBB_n^\c\setminus\Lambda})-\gamma_\Lambda^\Phi(\tilde{\omega}_\Lambda|\omega_{\BBB_n\setminus\Lambda}\zeta_{\BBB_n^\c\setminus\Lambda})| ~\xrightarrow{n\ra\infty}~ 0.$$
		\end{itemize}
		\item[(ii)] Let $\mu$ be a fully supported Borel probability measure on $\Omega$. If there exists a specification (in the sense of \textcolor{red}{\texttt{[Remark]}}) which is uniformly non-null, quasilocal and with which $\mu$ is consistent (in the sense of \textcolor{red}{\texttt{[Definition]}}), then there exists some $\Phi\in\BB^1(\Omega)$ so that $\mu\in\G_\Omega(\Phi)$.
		
\end{itemize}
\end{prop}

% *************************************************************************************************

\subsubsection{Coexistence of several Gibbs measures}

While the result in \textcolor{red}{\texttt{[Remark]}}(ii) tells us that for an UAC interaction, there's always a Gibbs measure, it tells us nothing else about the cardinality in $\G_\Omega(\Phi)$: in particular it doesn't guarantee that there's only one. In fact several Gibbs measures consistent with the same interaction may exist; in this case, we say that the system has a \textit{phase transition}. Given a model with some parameter(s), an important question is for which parameter values there's only one Gibbs measure and for which parameter there's several.

\textcolor{blue}{\begin{ex}[Ising]~\end{ex}}

The spaces of Gibbs measures do carry some nice properties:
\begin{prop}
Let $\Phi\in\BB^1(\Omega)$.
\begin{itemize}
	\item[(i)] $\G_\Omega(\Phi)$ is convex.
	\item[(ii)] Extremal measures of $\G_\Omega(\Phi)$ (denoted by $\extr(\G_\Omega(\Phi))$) are mutually singular.
	\item[(iii)] For each $\mu\in\G_\Omega(\Phi)$,
	$$\gamma_{\BBB_n}^\Phi(\pika|\omega) ~\xrightarrow{w}~ \mu, \quad \text{for}~\mu\text{-a.a.}~\omega\in\Omega.$$
	In other words, $\mu$ can be obtained as a thermodynamic limit with some fixed boundary condition. (See next section.)
\end{itemize}
\end{prop}

\textcolor{blue}{Before concluding this preliminary section, we present an important result, which is very commonly used to verify uniqueness of a Gibbs measure with respect to the interaction:}
\begin{thm}[Dobrushin's Uniqueness Condition]
Let $\Phi\in\BB^1(\Omega)$ and define for $x,y\in\Z^d$,
$$c_{x,y}^\Phi ~:=~ \sup_{\substack{\omega,\tilde{\omega}\in\Omega:\\\omega\equiv\tilde{\omega}~\text{off}~ y}}\|\gamma_x^\Phi(\pika|\omega)-\gamma_x^\Phi(\pika|\tilde{\omega})\|_\TV.$$
If
$$\sup_{x\in\Z^d}\sum_{y\in\Z^d}c_{x,y}^\Phi ~<~ \textcolor{purple}{1},$$
then $|\G_\Omega(\Phi)|=1$.
\end{thm}

In practice, the following criterion is useful in particular:

\begin{cor}
$\Phi\in\BB^1(\Omega)$ satisfies the Dobrushin's Uniqueness Condition as soon as
$$\sup_{x\in\Z^d}\sum_{\substack{\Lambda\Subset\Z^d:\\x\in\Lambda}}(|\Lambda|-1)\sup_{\omega,\tilde{\omega}}|\Phi_\Lambda(\omega)-\Phi_\Lambda(\tilde{\omega})| ~<~ \textcolor{purple}{2}.$$
We will call interaction satisfying this \textit{high-temperature interactions}.
\end{cor}

In practice, one may sometimes approach a collection of Hamiltonians, that differ from each other purely by a single-site interaction. A very useful fact, which will be of great importance in a later chapter on spin-flip dynamics, is that as soon as one of them satisfies \textcolor{red}{\texttt{[Eq in Cor]}}, so do all the other:
\begin{lem}
Suppose that $\Phi\in\BB^1(\Omega)$ satisfies \textcolor{red}{\texttt{Eq in Cor}}. Moreover, let $\Psi\in\BB^1(\Omega)$ be a \textit{single-site} interaction, i.e., $\Psi_\Lambda$ is non-zero only for $\Lambda=\set{x}$, $x\in\Z^d$. Then, $\Phi^*:=\Phi+\Psi$ satisfies \textcolor{red}{\texttt{[Eq in Cor]}} as well.
\end{lem}

\begin{proof}
By triangle inequality,
\begin{align*}
\sup_{x\in\Z^d}\sum_{\substack{\Lambda\Subset\Z^d:\\x\in\Lambda}}(|\Lambda|-1)\sup_{\omega,\tilde{\omega}}|\Phi_\Lambda^*(\omega)-\Phi_\Lambda^*(\tilde{\omega})| ~&\leq~ \sup_{x\in\Z^d}\sum_{\substack{\Lambda\Subset\Z^d:\\x\in\Lambda}}(|\Lambda|-1)\sup_{\omega,\tilde{\omega}}|\Phi_\Lambda(\omega)-\Phi_\Lambda(\tilde{\omega})| \\
&+~ \sup_{x\in\Z^d}\sum_{\substack{\Lambda\Subset\Z^d:\\x\in\Lambda}}(|\Lambda|-1)\sup_{\omega,\tilde{\omega}}|\Psi_\Lambda(\omega)-\Psi_\Lambda(\tilde{\omega})| \\
&<~ 2 \\
&+~ \sup_{x\in\Z^d}\sum_{\substack{\Lambda\Subset\Z^d:\\x\in\Lambda}}(|\Lambda|-1)\sup_{\omega,\tilde{\omega}}|\Psi_\Lambda(\omega)-\Psi_\Lambda(\tilde{\omega})|
\end{align*}
Using that for any $x\in\Z^d$ and $\Lambda\Subset\Z^d$ containing $x$, $\Psi_\Lambda\equiv 0$ as soon as $\Lambda\neq\set{x}$, we have that
$$\sum_{\substack{\Lambda\Subset\Z^d:\\x\in\Lambda}}(|\Lambda|-1)\sup_{\omega,\tilde{\omega}}|\Psi_\Lambda(\omega)-\Psi_\Lambda(\tilde{\omega})| ~=~ (|\!\set{x}\!|-1)\sup_{\omega,\tilde{\omega}\in\Omega}|\Psi_x(\omega)-\Psi_x(\tilde{\omega})|.$$
The right-hand side clearly equals zero, since $|\!\set{x}\!|=1$, yielding the conclusion.
\end{proof}

% *************************************************************************************************

\subsubsection{Thermodynamic limits}

For technical reasons, we conclude this preliminary section with a few words about thermodynamic limits, on which we have already hinted earlier and also borrowed the notion in \textcolor{red}{\texttt{[Remark]}}. \\

While definition is straightforward, it leads us to introduce a couple of notions beforehand.
\begin{df}
~
\begin{itemize}
	\item[(1)] We say that $(\Lambda_n)_{n\in\N}$, where $\Lambda_n\Subset\Z^d$ for each $n\in\N$, is a \textit{van Hove sequence}, if it satisfies the following conditions:
	\begin{itemize}
		\item[(i)] it is \textit{increasing}: $\Lambda_n\subset\Lambda_{n+1}$ for each $n\in\N$,
		\item[(ii)] it \textit{invades} $\Z^d$: $\bigcup_{n\geq 1}\Lambda_n=\Z^d$, and
		\item[(iii)] $\lim_{n\ra\infty}\frac{|\partial\Lambda_n|}{|\Lambda_n|}=0$. \textcolor{blue}{Definition of $\partial\Lambda$?}
	\end{itemize}
	We denote van Hove convergence by $\Lambda_n\Uparrow\Z^d$.
	\item[(2)] A function $f:\Omega\ra\R$ is said to be \textit{local} if there's $\Delta\Subset\Z^d$, such that for any choice of $\omega,\tilde{\omega}\in\Omega$, 
	$$\omega(x)=\tilde{\omega}(x)~\forall x\in\Delta ~\Longrightarrow~ f(\omega)=f(\tilde{\omega}).$$
	We sometimes write $\supp(f)$ for the minimal such $\Delta$. We denote the collection of all local functions $f:\Omega\ra\R$ by $\Loc(\Omega)$.
	\item[(c)] Given (probability) measures $\mu,\mu_1,\mu_2,\ldots$ on $\Omega$, we say that $(\mu_n)_{n\in\N}$ converges \textit{weakly} to $\mu$, writing $\mu_n\xrightarrow{w}\mu$, if
	$$\int_\Omega f\,\d\mu_n ~\ra~ \int_\Omega f\,\d\mu, \quad \forall f\in\Loc(\Omega).$$
\end{itemize}
\end{df}

Now we are fully equipped to define thermodynamic-limit Gibbs measures:
\textcolor{purple}{
\begin{df}
We say that $\mu$ is a thermodynamic-limit Gibbs measure for $\Phi\in\BB^1(\Omega)$, if there exists a boundary condition $\omega\in\Omega$, such that
$$\gamma_{\Lambda_n}^\Phi(\pika|\omega_{\Lambda^\c}) ~\xrightarrow{w}~ \mu \quad \text{as}~n\ra\infty,$$
for some van Hove sequence $(\Lambda_n)_{n\in\N}$.
\end{df}
}

\textcolor{purple}{
\begin{rem}
Using the same procedure, we may also define non-Gibbs thermodynamic-limit measures, using a more general specificaiton $\gamma$ (in the sense of \textcolor{red}{\texttt{[Remark]}}), not necessarily as configurations on vertices (but for example on edges, as will be seen later).
\end{rem}
}

\textcolor{blue}{
\begin{ex}[Ising: +,- vs Dobrushin]
\end{ex}
}

% #################################################################################################

\subsection{Renormalisation group pathologies}

We now direct our attention to the source of the questions that inspired the theory which is presented in this thesis. Those can be traced back to the seminal paper of Aernout van Enter, Roberto Fern\'andez and Alan Sokal, carrying the title \textit{Regularity Properties and Pathologies of Position-Space Renormalization-Group Transformations}. \\

\textcolor{blue}{Informally, the problem goes as follows.} Consider (measure) spaces $\Omega,\Omega'$ of configurations on a (for now) finite volume $\Lambda$ and suppose $\Omega$ is equipped with a probability measure $\mu$. A renormalization-group (RG) map is given as a a probability kernel $T$ from $\Omega$ to $\Omega'$. $T$ can be either a deterministic or a stochastic map.\footnote{In the deterministic case, $T(\pika,\omega')$ corresponds to Dirac measure $\delta_{\omega'}$.} This maps gives us a natural measure $\mu'$ on $\Omega'$, which is given by
$$\mu'(\omega') ~=~ (\mu T)(\omega') ~:=~ \sum_{\omega\in\Omega}\mu(\omega)T(\omega,\omega'), \quad \omega'\in\Omega'.$$
Note that if $T$ is deterministic, given by a map $\tilde{T}:\Omega\ra\Omega'$, $\mu T$ would correspond to the push-forward measure $\mu\circ \tilde{T}^{-1}$. Thus, the RG map is without a problem defined between measures. In applications, however, an alternative route is usually taken. In particular, one defines the RG map as a map $\RR$ between (suitable) Hamiltonians: given a Hamiltonian $\H$, with which $\mu$ is consistent $(\mu\propto e^{-\H})$, $\RR$ assigns $\H$ the Hamiltonian $\H'$, which corresponds to $\mu'=\mu T$ ($\mu'\propto e^{-\H'}$). In particular, $\H'$ may be obtained via the formula
$$\H'(\omega') ~=~ (\RR\H)(\omega') ~=~ -\log\sum_{\omega\in\Omega} e^{-\H(\omega)}T(\omega,\omega)' + C,$$
with $C$ a constant. The interplay between $T$ and $\RR$ is captured in the following diagram:
$$\begin{tikzcd}
\H & {\H'} \\
	\mu & {\mu'}
	\arrow["\RR", from=1-1, to=1-2]
	\arrow[from=1-1, to=2-1]
	\arrow["T"', from=2-1, to=2-2]
	\arrow[from=2-2, to=1-2]
\end{tikzcd}$$
This nice formalism, however, fails to translate when we instead take an infinite volume $\Lambda$, for example $\Z^d$. First clear problem is that there might be several Gibbs measures consistent with $\H$, implying that $\H\mapsto\mu$ is a multi-valued map. \textcolor{purple}{While $\mu'$ can still be defined rather simply,
$$\mu'(A') ~:=~ \int_\Omega T(\omega,A)\,\d\mu(\omega),$$}
it yields a new problem, which has \textcolor{purple}{first demonstrated} in \textcolor{red}{\texttt{[Isreal paper]}} and widely argued in \textcolor{red}{\texttt{[vEFS]}}: $\mu'$ might fail to be Gibbsian, that is, it might fail to be consistent with any suitable Hamiltonian, yielding that the map $\mu'\mapsto\H'$ might not be defined for all image measures $\mu'$. This led authors of \textcolor{red}{\texttt{vEFS}} to reformulate the formalism, for which several results were proven and many new open questions posed. \\

\textcolor{blue}{This thesis in particular is concerned with a established conjecture, stated in the following chapter, regarding the sufficient condition for Gibbsianity of $\mu'$, in a certain deterministic setting, which was already heavily explored in \textcolor{red}{\texttt{[Berghout]}}.} \\

To conclude this section, we provide two examples of RG transformations, one deterministic (decimation) and one stochastic (majority vote).

\textcolor{blue}{
\begin{ex}[Decimation of Ising model]
~
\end{ex}
}

\textcolor{blue}{
\begin{ex}[Majority vote on Ising model]
~
\end{ex}
}


% #################################################################################################

\pagebreak

% #################################################################################################
% #################################################################################################
% #################################################################################################

\section{Fuzzy Gibbs formalism}

\pagebreak

% #################################################################################################
% #################################################################################################
% #################################################################################################

\section{(Non-)Gibbsianity of fuzzy Potts model}

This chapter aims to introduce the fuzzy Potts model and provide an alternative, independent proof of H\"aggstr\"om's result \textcolor{red}{\texttt{[reference]}} about conditions for its Gibbsianity, using the results due to Berghout and Verbitskiy \textcolor{red}{\texttt{[reference]}}. Moreover, it introduces the notion of random cluster representations, a powerful tool in the theory of Potts model, which is used in the proof.

% #################################################################################################

\subsection{Classical Potts model}

In this introductory section of the chapter, we define the classical Potts model, with respect to which the fuzzy Potts model will be later defined. We also state the celebrated result the existence of phase transition with respect to the inverse temperature parameter. \\

The Potts model is a natural generalization of (ferromagnetic) Ising model with zero external field ($h=0$), where instead of two possible values ($+/-$ or $1/-1$ in the context of the Ising model), the spins may assume $q\geq 2$ different values, taking $\set{1,\ldots,q}$ to be the state \textcolor{blue}{(spin)} space, and consequently $\Omega=\set{1,\ldots,q}^{\Z^d}$ to be the configuration space. As in the case of the Ising model, the interaction $\Phi$ is defined for $\textcolor{blue}{A}=\set{x,y}$, with $x\sim y$ (i.e., $|x-y|=1$) and is given by
$$\Phi(\omega_x,\omega_y) ~=~ 2\1_{\set{\omega_x\neq \omega_y}}-1 ~=~ \begin{cases}
1: ~&\omega_x\neq\omega_y, \\
-1: ~&\omega_x=\omega_y.
\end{cases}$$
Clearly, taking $q=2$ and identifying $\set{0,1}$ with $\set{-,+}$, one obtains the zero external field Ising model. The above interaction yields the following finite volume Hamiltonian expression: given $\Lambda\Subset\Z^d$, $\omega_\Lambda\in\Omega_\Lambda$ and $\sigma_{\Lambda^\c}\in\Omega_{\Lambda^\c}$, we have
$$\H_\Lambda(\omega_\Lambda\sigma_{\Lambda^\c}) ~=~ \beta\sum_{(x,y)\in\tilde{\Lambda}}(2\1_{\set{\omega_x\neq\omega_y}}-1)+\beta\sum_{(x,y)\in\partial\tilde{\Lambda}}(2\1_{\set{\omega_x\neq\sigma_y}}-1),$$
where $\tilde{\Lambda}:=\set{(x,y):x\sim y,\,x,y\in\Lambda}$ and $\partial\tilde{\Lambda}:=\set{(x,y):x\sim y,\,x\in\Lambda,y\in\Lambda^\c}$. We might, however, take a simpler equivalent interaction, which we justify shortly, in a remark. In the spirit of thermodynamical formalism, the system of Hamiltonians $(\H_\Lambda)_{\Lambda\Subset\Z^d}$ yields admits a specification $\gamma=(\gamma_\Lambda)_{\Lambda\Subset\Z^d}$, where for each $\Lambda\Subset\Z^d$ we have
\begin{align*}
\gamma_\Lambda:\BB(\Omega_\Lambda)\times\Omega_{\Lambda^\c}&\ra (0,1) \\
(\set{\omega_{\Lambda}},\sigma_{\Lambda^\c})&\mapsto\gamma_{\Lambda}(\omega_{\Lambda}|\sigma_{\Lambda^\c})=\frac{1}{\ZZ}\exp\!\oklepaj{-\H_\Lambda(\omega_\Lambda\sigma_{\Lambda^\c})},
\end{align*}
where $\ZZ_\Lambda=\sum_{\tilde{\omega}_\Lambda\in\Omega_\Lambda}\exp\!\oklepaj{-\H_\Lambda(\tilde{\omega}_{\Lambda}\sigma_{\Lambda^\c})}$.

\begin{rem}
\begin{itemize}
	\item[(1)] Both $\H_\Lambda$ and $\ZZ_\Lambda$, depend not only on $\Lambda$ but also on $q$ and $\beta$.
	\item[(2)] Notice that $$\H_{\Lambda}(\omega_{\Lambda}\sigma_{\Lambda^\c})=2\beta\sum_{(x,y)\in\tilde{\Lambda}}\1_{\set{\omega_x\neq\omega_y}} + 2\beta\sum_{(x,y)\in\partial\tilde{\Lambda}}\1_{\set{\omega_x\neq\sigma_y}}-\beta(|\tilde{\Lambda}|+|\partial\tilde{\Lambda}|),$$
	where $C_{\beta,\Lambda}:=\beta(|\tilde{\Lambda}|+|\partial\tilde{\Lambda}|)$ is a  constant independent of $\omega_{\Lambda},\sigma_{\Lambda^\c}$, giving us
	\begin{align*}
	\gamma_{\Lambda}(\omega_{\Lambda}|\sigma_{\Lambda^\c}) ~&=~ \frac{\exp(-\H_{\Lambda}(\omega_{\Lambda}\sigma_{\Lambda^\c}))}{\sum_{\tilde{\omega}_{\Lambda}\in\Omega_{\Lambda}}\exp(-\H_{\Lambda}(\tilde{\omega}_{\Lambda}\sigma_{\Lambda^\c}))} \\
	&=~ \frac{\exp\!\oklepaj{-2\beta\!\oglati{\sum_{(x,y)\in\tilde{\Lambda}}\1_{\set{\omega_x\neq\omega_y}} + \sum_{(x,y)\in\partial\tilde{\Lambda}}\1_{\set{\omega_x\neq\sigma_y}}}}\exp(C_{\beta,\Lambda})}{\exp(C_{\beta,\Lambda})\sum_{\tilde{\omega}_{\Lambda}\in\Omega_{\Lambda}}\exp\!\oklepaj{-2\beta\!\oglati{\sum_{(x,y)\in\tilde{\Lambda}}\1_{\set{\tilde{\omega}_x\neq\tilde{\omega}_y}} + \sum_{(x,y)\in\partial\tilde{\Lambda}}\1_{\set{\tilde{\omega}_x\neq\sigma_y}}}}} \\
	&=~ \frac{\exp\!\oklepaj{-2\beta\!\oglati{\sum_{(x,y)\in\tilde{\Lambda}}\1_{\set{\omega_x\neq\omega_y}} + \sum_{(x,y)\in\partial\tilde{\Lambda}}\1_{\set{\omega_x\neq\sigma_y}}}}}{\sum_{\tilde{\omega}_{\Lambda}\in\Omega_{\Lambda}}\exp\!\oklepaj{-2\beta\!\oglati{\sum_{(x,y)\in\tilde{\Lambda}}\1_{\set{\tilde{\omega}_x\neq\tilde{\omega}_y}} + \sum_{(x,y)\in\partial\tilde{\Lambda}}\1_{\set{\tilde{\omega}_x\neq\sigma_y}}}}},
	\end{align*}
	so the same specification would be obtained by selecting an alternative interaction and system of (finite volume) Hamiltonians
	\begin{align*}
	\Phi'(\omega_x,\omega_y) ~&=~ 2\1_{\set{\omega_x\neq\omega_y}}, \\
	\H_{\Lambda}'(\omega_{\Lambda}\sigma_{\Lambda^\c}) ~&=~ 2\beta\sum_{(x,y)\in\tilde{\Lambda}}\1_{\set{\omega_x\neq\omega_y}}+2\beta\sum_{(x,y)\in\partial\tilde{\Lambda}}\1_{\set{\omega_x\neq\sigma_y}}.
	\end{align*}
	For the sake of simplicity, from now on, we use will use $\Phi'$ and $(\H_\Lambda')_{\Lambda\Subset\Z^d}$ in our computations instead, and simply denote them by $\Phi$ and $(\H_{\Lambda})_{\Lambda\Subset\Z^d}$.
	\item[(3)] The interaction we initially used, is often also written in the literature as 
	$$\Phi(\omega_x,\omega_y) ~=~ 1-2\1_{\set{\omega_x=\omega_y}}.$$
	Had we used this version, we would in point (2) obtain $\Phi'(\omega_x,\omega_y)=-2\1_{\set{\omega_x=\omega_y}}$ instead.
	\item[(4)] There are several other interactions that equivalently define the Potts model. \textcolor{blue}{Give examples.} \textcolor{blue}{$\1_{\set{\omega_x\neq\omega_y}}$ and $-\1_{\set{\omega_x=\omega_y}}$?}
\end{itemize}
\end{rem}

In the spirit of thermodynamical formalism, the (infinite volume) Gibbs measure for $q$-state Potts model on $\Omega$ with inverse temperature $\beta$ is any probability measure on $\Omega$, whose finite-volume conditional probabilities agree with the specification $\gamma$, that is, a measure $\mu\in\M_1(\Omega)$ such that for each $\Lambda\Subset\Z^d$ and $\omega$, one has
$$\mu(\omega~\text{on}~\Lambda|\sigma~\text{off}~\Lambda) ~=~ \gamma_\Lambda(\omega_\Lambda|\sigma_{\Lambda^\c}),$$
for $\mu$-a.e.~$\sigma\in\Omega$.

\begin{rem}
Notice that for any fixed $\sigma_{\Lambda^\c}\in\Omega_{\Lambda^\c}$, the map $\gamma_\Lambda(\pika|\sigma_{\Lambda^\c})$ is a probability measure on $\Omega_\Lambda$, which we can denote as $\mu_\Lambda^\sigma=\mu_{q,\beta,\Lambda}^\sigma$. We can, in this case, restate $\mu\in\M_1(\Omega)$ to be Gibbs measure for $q$-state Potts model with inverse temperature $\beta$ if for each $\Lambda\Subset\Z^d$ and $\mu$-a.e.~$\sigma\in\Omega$, 
$$\mu(\pika~\text{on}~\Lambda|\sigma~\text{off}~\Lambda) ~=~ \mu_{q,\beta,\Lambda}^\sigma.$$
\end{rem}

The Potts model enjoys \textcolor{blue}{a similar} phase transition result as the Ising model, as is stated in the following theorem.

\begin{thm}[\textcolor{blue}{By whom?ACCN?}]
For $q$-state Potts model on $\Z^d$ with $d\geq 2$, there exists a critical inverse temprature $\beta_c=\beta_c(d,q)\in(0,\infty)$ such that
\begin{itemize}
	\item[(i)] for each $\beta<\beta_c$ there is a unique Gibbs measure, and 
	\item[(ii)] for each $\beta>\beta_c$, there exist $q$ mutually singular Gibbs measures.
\end{itemize}
\end{thm}

It is worth mentioning that the mutually singular Gibbs measures in the $\beta>\beta_c$ regime are precisely the measures $\mu_{q,\beta}^1,\ldots,\mu_{q,\beta}^q$, where for $i\in\set{1,\ldots,q}$, $\mu_{q,\beta}^i$ is the thermodynamical limit
$$\mu_{q,\beta}^i ~=~ \lim_{\Lambda\Uparrow\Z^d}\mu_{q,\beta,\Lambda}^{\mathsf{i}},$$
where $\mathsf{i}=\set{i}^{\Z^d}$ is the constant boundary condition.

% #################################################################################################

\subsection{Fuzzy Potts model}

\textcolor{blue}{In this introductory section of the chapter, we define the fuzzy Potts model and state the celebrated result about its (non-)Gibbsianity, due to H\"aggstr\"om \textcolor{red}{\texttt{[reference]}}. Moreover, we explain the strategy and structure of our alternative proof of (part of) the said result.} \\

Consider the Potts model with spin space $\set{1,\ldots,q}$, $q\geq 2$, on lattice $\L$, say $\L=\Z^d$, which defines a model on $\Omega=\set{1,\ldots,q}^\L$. The \textit{fuzzy Potts model} is defined by considering some integer $1<s<q$, so that the spin space is $\set{1,\ldots,s}$ and the whole model defined on $\Sigma=\set{1,\ldots,s}^\L$. Moreover, we consider a vector $\r=(r_1,\ldots,r_s)\in\N^s$, such that $r_1+\ldots+r_s=q$ and define a fuzzy transformation $\pi_\r:\set{1,\ldots,q}\ra\set{1,\ldots,s}$ by putting 
$$\pi_\r(a) ~:=~ \begin{cases}
1: ~&1\leq a\leq r_1,\\
2: ~&r_1+1\leq a\leq r_1+r_2 \\
\cdots \\
n: ~&\sum_{i=1}^{n-1} r_i + 1\leq a\leq \sum_{i=1}^n r_i,\\
\cdots \\
s: ~&\sum_{i=1}^{s-1}r_i + 1\leq a\leq q,
\end{cases}$$
i.e., $\pi_a=n$ iff $a\in(\sum_{i=1}^{n-1}r_i,\sum_{i=1}^n r_i]\cap \N$, $n\in\set{1,\ldots,s}$. In other words, the entire fuzzy map $\pi=\pi_\r$ is encoded by a single $s$-vector $\r$. \\

Fixing $q\geq 2$, $\beta\geq 0$ and writing $\mu_{q,\beta}^{\Z^d,\#}$ for the Gibbs measure of the Potts model on $\set{1,\ldots,q}^{\Z^d}$ for boundary condition $\#\in\set{0,\ldots,q}$ with inverse temperature $\beta$, the fuzzy transformation $\pi_\r$ induces the fuzzy Gibbs measure 
$$\nu_{q,\beta,\r}^{\Z^d,\#} ~:=~ \mu_{q,\beta}^{\Z^d,\#}\circ\pi_\r^{-1}.$$
Of great interest is the potential Gibbsianity of such measure. \textcolor{blue}{\texttt{Something about the H\"aggstr\"om's result blahblahblah.}} Recall that for \textcolor{purple}{$q\geq 2$ and $d\geq 2$}, there exists $\beta_c(d,q)\in(0,\infty)$ such that for each $\beta<\beta_c(d,q)$, $\mu_{q,\beta}^{\Z^d,0}=\ldots=\mu_{q,\beta}^{\Z^d,q}$, i.e., there is a unique Gibbs measure of the Potts model on $\set{1,\ldots,q}^{\Z^d}$ with inverse temperature $\beta$, while for each $\beta>\beta_c(d,q)$ there are $q$ mutually singular Gibbs measures \textcolor{purple}{\texttt{[q+1?]}}.

\begin{thm}[H\"aggstr\"om, 2003, \textcolor{red}{\texttt{[reference]}}]
Let $d\geq 2$, $q\geq 3$, $\#\in\set{0,\ldots,q}$ and $\r=(r_1,\ldots,r_s)$ with $1<s<q$, $r_1+\ldots+r_s=q$, and write $r^*=\min_{1\leq i\leq s}r_i$. Consider a fuzzy Gibbs measure $\nu_{q,\beta,\r}^{\Z^d,\#}=\mu_{q,\beta}^{\Z^d,\#}\circ\pi_{\r}^{-1}$.
\begin{itemize}
	\item[(i)] For each $\beta<\beta_c(d,r^*)$, the measure $\nu_{q,\beta,\r}^{\Z^d,\#}$ is a Gibbs measure.
	\item[(ii)] \textcolor{blue}{\texttt{The non-Gibbs part.}}
\end{itemize}
\end{thm}

\begin{rem}
\textcolor{blue}{\texttt{Remark about the ordering of $\beta_c(d,r_1),\ldots,\beta_c(d,r_s)$, given the ordering of $r_1,\ldots,r_s$. Explain that this condition gives uniqueness of Gibbs measure on each $\set{1,\ldots,r_i}^{\Z^d}$.}}
\end{rem}

In light of the theory from the previous chapter, we are particularly interested in part (i) of the theorem, the Gibbs regime. The van Enter-Fern\'andez-Sokal hypothesis, would suggest that, since for $\beta<\beta_c(d,r^*)$ the Gibbs property is preserved, \textcolor{purple}{each $\mu_{q,\beta}^{\Z^d,\#}$ should admit a continuous disintegration in terms of $\nu_{q,\beta,\r}^{\Z^d,\#}$}. Moreover, proving the latter would -- applying the result of \textcolor{red}{\texttt{[theorem reference]}} -- constitute an alternative and independent proof of \textcolor{red}{\texttt{[Theorem 2.1]}}.(i). \\

Given \textcolor{red}{\texttt{Theorem reference}}, it is sufficient to verify (for a fixed $\beta<\beta_c(d,r^*))$, that for each $\sigma\in\Sigma=\set{1,\ldots,s}^{\Z^d}$, there is a unique Gibbs measure for $q$-Potts model with inverse temperature beta on the fibre $\Omega_\sigma=\pi_\r^{-1}(\sigma)$, i.e., that
$$|\G_{\Omega_\sigma}(\Phi_{q,\beta}^\Potts)| ~=~ 1, \quad \forall \sigma\in\Sigma.$$
Luckily, one can express the fibres in a rather nice way, allowing for an easier procedure. Given $\sigma\in\Sigma$, we simply have
$$\Omega_\sigma ~=~ \pi_{\r}^{-1} ~=~ \prod_{i\in\Z^d}\pi_{\r}^{-1}(\sigma_i).$$
Writing $\A_j:=\pi_{\r}^{-1}(j)$ and $U_j:=\set{i\in\Z^d:\pi_{\r}(\sigma_i)=j}$, $j=1,\ldots,s$, we could also write
$$\Omega_\sigma ~=~ \textcolor{purple}{\bigotimes_{j=1}^s\A_j^{U_j}} ~:=~ \prod_{i\in\Z^d}\begin{cases}\A_1:~&i\in U_1,\\
\cdots\\
\A_s:~&i\in U_s.\end{cases}$$
One way of proving the uniqueness of $q$-Potts Gibbs measure on such $\Omega_\sigma$ for \textcolor{blue}{\texttt{an appropriate}} inverse temperature $\beta$, is via the following two steps (so far stated informally):
\begin{itemize}
	\item[(1)] Given the assumption on $\beta$, we know that for each $j=1,\ldots,s$, there is a unique Gibbs measure for Potts model on $\A_j^{\Z^d}$ with inverse temperature $\beta$. We wish to show that this implies also uniqueness of Gibbs measure for Potts model on $\A_j^{U_j}$, given the same inverse temperature.
	\item[(2)] Given the uniqueness of Gibbs measure for Potts model on $\A_j^{U_j}$ with inverse temperature $\beta$ for all $j=1,\ldots,s$, we need to show that this implies the uniqueness of Gibbs measure for Potts model on $\bigotimes_{j=1}^s \A_j^{U_j}$, given the same inverse temperature.
\end{itemize}
\textcolor{blue}{\texttt{Here, I will remark that it is enough to check this for $s=2$, and proceed to restate the above points more formally, in terms of two propositions.}}

% #################################################################################################

\subsection{Preliminaries, part 1: stochastic domination}

In this and the next section, we will introduce some tools, which will be used to prove \textcolor{red}{\texttt{[Proposition reference]}}. The section will cover the basics on stochastic domination, a well-established tool in mathematical statistical mechanics. In the section that follows, we introduce the theory of random cluster representations, which are important in study of the Potts model in particular. 


Assume firstly that $\A\subset\R$ is linearly ordered; in our case, we are generally assuming $\A$ to be finite, though this theory can be extended to closed subsets of $\R$. Then linear order of $\A$ induces a natural (coordinatewise) partial order on $\Omega=\A^\L$: given two configurations $\xi,\xi'\in\Omega$, we write
$$\xi\preceq\xi' ~\iff~ \xi(i)\leq\xi'(i),~\forall i\in\L.$$
This allows us to introduce a notion of increasing functions:

\begin{df}[Increasing functions and events]
~
\begin{itemize}
	\item[(1)] We say that a function $f:\Omega\ra\R$ is \textit{increasing} (or \textit{non-decreasing}), if for each $\xi,\xi'\in\Omega$, $\xi\preceq\xi'$ implies $f(\xi)\leq f(\xi')$. 
	\item[(2)] Equipped with the previous notion, we say that an event $A$ (or a simply $A\subset\Omega$ measurable) is \textit{increasing} if $\1_A$ is an increasing function.
\end{itemize}
\end{df}

\begin{ex}
\textcolor{blue}{\texttt{Percolation probability.}}
\end{ex}

 This allows us to define a certain partial order on $\M_1(\Omega)$, the set of probability measures on $\Omega$. \textcolor{blue}{\texttt{Some comment about motivation for defining stochastic domination/ordering on measures?}}
 
\begin{df}[Stochastic domination]
Let $\mu$ and $\mu'$ be probability measures on $\Omega$. We say that $\mu$ is \textit{stochastically dominated} by $\mu'$, writing $\mu\preceq_\D\mu'$, if
$$\int f\,\d\mu ~\leq~ \int f\,\d\mu', \quad \forall f\in\BM(\Omega,\R).$$
\end{df}

\textcolor{blue}{\texttt{Somethingsomething coupling}}

\begin{df}[Coupling]
Given two probability measures $\mu,\mu'$ on $\Omega$, their \textit{coupling} is defined to be a probability measure $\PP$ on $\Omega\times\Omega$, such that its marginals agree with $\mu$ and $\mu'$, that is
$$\PP(\set{(\xi,\xi'):\xi\in A}) ~=~ \mu(A) \quad \text{and} \quad \PP(\set{(\xi,\xi'):\xi'\in B}) ~=~ \mu'(B).$$
\end{df}

\textcolor{blue}{\texttt{The rest of this section will depend on which argument exactly I will use in the proof, but both Strassen and Holley could be introduced, as a part of a ``general introduction''.}}

% #################################################################################################

\subsection{Preliminaries, part 2: random cluster representations}

% #################################################################################################

\pagebreak

% #################################################################################################
% #################################################################################################
% #################################################################################################

\section{(Non-)Gibbsianity of random spin-flip dynamics}

In this chapter, we introduce the random spin-flip dynamics model, which was explored by van Enter, Fern\'andez, den Hollander and Redig in \textcolor{red}{\texttt{[reference]}}, and proceed to demonstrate, that a special case of this model may be seen as a fuzzy Gibbs model. We present some results about conditions for Gibbsianity and \textcolor{blue}{demonstrate that in the fuzzy Gibbs case, the proofs are analogue to \texttt{our} approach, which in turn verifies, that the example does not contradict the van Enter-Fern\'andez-Sokal hypothesis}.

% #################################################################################################

\subsection{The general model}

We proceed to introduce the model, as defined in \textcolor{red}{\texttt{[reference]}}, with some minor changes in notation. In principle, we are interested in a (probability) measure valued stochastic process $(\nu_t)_{t\geq 0}$, starting from a Gibbs measure. In particular the question the original paper explores is how the conditions on the starting measure and on process-inducing dynamics influence the (non-)Gibbsianity of $\nu_t$, $t>0$. \\

Define first $\Omega_0:=\set{-1,+1}^{\Z^d}$ to be the configuration space. We first consider dynamics on configurations, which is governed by spin-flip rates $\Z^d\times\Omega_0\ni(x,\omega)\mapsto c(x,\omega)$, which satisfy the following conditions:
\begin{itemize}
	\item[(i)] Finite range: for any $x\in\Z^d$, the map $c_x:=(\omega\mapsto c(x,\omega))$ is a local function of $\omega$, with $\mathrm{diam}(\D_{c_x})\leq R<\infty$.
	\item[(ii)] Translation invariance: \textcolor{blue}{for any $x\in\Z^d$, $\tau_x c_0=c_x$}. % define $\tau_x$
	\item[(iii)] Strict positivity: for all $x\in\Z^d$ and $\omega\in\Omega$, $c(x,\omega)>0$.
\end{itemize}

\begin{rem}
It is a direct consequence of assuming (i)-(iii), that there's $m,M\in(0,\infty)$, such that 
$$0 ~<~ m ~\leq~ c(x,\omega) ~\leq~ M ~<~ \infty, \quad \forall x\in\Z^d,\,\omega\in\Omega.$$
\end{rem}

The idea behind the dynamics is as follows: one starts with a (possibly random) configuration $\omega_0$ at time $0$ and flips (random) spins at random times, which are being governed by $(c_x)_{x\in\Z^d}$. That is, site $x\in\Z^d$ is being flipped at \textcolor{blue}{(exponential?)} rate $c_x$, which in general depends on the current configuration, through its value at $x$ and a certain neighbourhood of $x$. Note that unless $\mathrm{diam}(\D_{c_x})=0$ for each $x\in\Z^d$ (i.e., $c(x,\omega)=c(x,\omega(x))$), the flips of individual sites are not independent.\footnote{This scenario, however, will be the one we will focus on later on.} \\

The goal now is to describe a semigroup $(S(t))_{t\geq 0}$ that will describe the dynamics induced by $(c_x)_{x\in\Z^d}$. This motivates us to define a generator $L$ on the space of local functions; writing $\omega^x$ for the configuration that agrees with $\omega$ off $x$ and with $-\omega$ on $x$, we can define
$$Lf ~=~ \sum_{x\in\Z^d}c_x(f(\omega^x)-f(\omega)), \quad f\in\Loc.$$ It is known \textcolor{red}{\texttt{[reference?]}} that the closure of $L$ on $\mathcal{C}(\Omega_0)$ constitutes a generator of a unique Feller process $(\omega_t)_{t\geq 0}$, with $\P_\omega$ the corresponding path-measure, conditional on $\omega_0=\omega$. Furthermore, we obtain the associated semigroup $(S(t))_{t\geq 0}$, which is given by
$$S(t) ~=~ \exp(tL), \quad t\geq 0.$$
It is precisely this semigroup, that in turn also gives us dynamics on measures. To be precise, $(S(t))_{t\geq 0}$ acts on probability measures via	
$$\int_{\Omega_0} [S(t)f](\omega)\,\d\nu(\omega) ~=~ \int_{\Omega_0}f(\omega)\,\d[\nu S(t)](\omega), \quad f\in\Loc,\,\nu\in\M_1(\Omega),\,t\geq 0.$$
In a more explicit language, the measure $\nu S(t)$ assigns a measurable set $A\subset\Omega_0$ a value $[\nu S(t)](A)=\int S(t)\1_A\,\d\nu$. \\

The interpretation turn out to be very intuitive. Letting $\omega_0\in\nu$ be the initial configuration and running the spin-flip dynamics the measure, then
$$\nu S(t) ~=~ \Law(\omega_t), \quad \forall t\geq 0.$$ 

Of central interest is the situation where $\omega_0\sim\mu$, where $\mu$ is a Gibbs measure, that is, $\mu\in\G_{\Omega_0}(\Phi)$ for some translation invariant, finite range \textcolor{blue}{good} interaction $\Phi$. It is also worth noting, that the dynamics induced by $(c_x)_{x\in\Z^d}$ \textcolor{blue}{admits (was this assumption or fact)} a $L$-invariant\footnote{$\int Lf\,\d\rho=0$ for each $f\in\Loc$.} Borel probability measure $\rho$ on $\Omega_0$ which is \textit{reversible}, that is,
$$\int_{\Omega_0}(Lf)g\,\d\rho ~=~ \int_{\Omega_0}f(Lg)\,\d\rho, \quad \forall f,g\in\Loc.$$
Writing $\rho^x$ for the law of $\omega^x$ where $\omega\sim\rho$, the reversibility of $\rho$ for $L$-induced spin flip dynamics is equivalent to
$$\frac{\d\rho^x}{\d\rho} ~=~ \frac{c(x,\omega)}{c(x,\omega^x)}, \quad \forall x\in\Z^d,\,\sigma\in\Omega_0,$$
which implies that there exists a continuous version of Radon-Nikod\'ym derivative $\frac{\d\rho^x}{\d\rho}$. It then follows from Proposition 2.2.~in \textcolor{red}{\texttt{[reference]}} that there exists a \textcolor{blue}{good} interaction $\Psi_\rho$ such that $\rho\in\G_{\Omega_0}(\Psi_\rho)$. Since we assumed $(c_{x})_{x\in\Z^d}$ to have finite range and be translation invariant, we can choose $\Psi_\rho$ to be of finite range and translation invariant as well.\\

The most general result given in the paper is the following:

\begin{thm}
Let $\mu\in\G_{\Omega_0}(\Phi)$ be the law of the original configuration and $\rho\in\Phi(\Psi)$ the reversible measure associated with $(c_x)_{x\in\Z^d}$-dynamics, with $\Phi,\Psi$ finite range \textcolor{blue}{and translation invariant}. Then there exists $t_0=t_0(\mu,\rho)>0$, such that $\mu S(t)$ is Gibbsian for each $0\leq t\leq t_0$. 
\end{thm}

\textcolor{blue}{\texttt{A brief comment.}}
 
% #################################################################################################

\subsection{Infinite-temperature dynamics}

A particularly nice setting, which my much nicer computational prospects is one of ``infinite-temperature'' dynamics, which means simply that spin flips of sites are independent of each other, i.e., the path measure $\P_\omega$ may be expressed as a product measure,
$$\P_\omega ~=~ \bigotimes_{x\in\Z^d}\P_{\omega(x)}.$$
This regime corresponds to $c_x$ depending on configuration only through its values at site $x$, that is, $\mathrm{diam}(\D_{c_x})=0$ for each $x\in\Z^d$. We will see why this model is computationally much simpler in the next section. \\

In this section, we will present some results from \textcolor{red}{\texttt{[reference]}} regarding the Gibbsianity of $(\mu S(t))_{t\geq 0}$ as well as present their proofs, which we will in the next section argue to be \textcolor{blue}{almost analogous} to our approach. \\

Before that, we present a result that is the central tool used in those proofs. If $\mu\in\G_{\Omega_0}(\Phi)$ is the law of the initial configuration, write $\H=(\H_\Lambda)_{\Lambda\Subset\Z^d}$ for the Hamiltonians consistent with $\Phi$. In all the proofs that will be presented, one argues by considering the distributions $\hat{\mu}_t$ of random variables of form $(\omega_0,\omega_t)$, $t\geq 0$. Formally, $\hat{\mu}_t$ enjoys the following correspondence with $\mu$ and $S(t)$:
$$\int_{\Omega_0^2} f(\sigma)g(\eta)\,\d\hat{\mu}_t(\sigma,\eta) ~=~ \int_{\Omega_0}f(\omega)[S(t)g](\omega)\,\d\mu(\omega), \quad f,g\in\Loc.$$
Technically speaking, due to its definition as a joint distribution, $\hat{\mu}_t$ has ``better chances'' of being Gibbsian than $\mu S(t)$. \textcolor{blue}{\texttt{(Elaboration? Any implication?)}} Assuming that it is indeed Gibbsian and writing $(p_t^x)_{t\geq 0}$ for the transition kernel associated with the dynamics at site $x$, the Hamiltonians associated with $\hat{\mu}_t$ may be expressed as
$$\H_\Lambda^{(t)}(\sigma_\Lambda,\eta_\Lambda) ~=~ \H_\Lambda(\sigma_\Lambda) - \sum_{x\in\Lambda}\log p_t^x(\sigma(x),\eta(x)), \quad \sigma,\eta\in\Omega_0.$$
Another assumption that authors make in this section is, that the generators of local spin-sites $(L_x)_{x\in\Z^d}$ are independent of $x$ (i.e., are all the same), and are of form
$$L_x ~=~ \frac{1}{2}\begin{pmatrix}
-1+\varepsilon & 1-\varepsilon \\
1+\varepsilon & -1-\varepsilon
\end{pmatrix}, \quad 0\leq \varepsilon<1,$$
which indeed yields that rates $(p_t^x)_{x\in\Z^d}$ are also independent of $x$, hence all the same.\\

The following result is the first part of the Proposition 3.7.~in \textcolor{red}{\texttt{[reference]}}:
\begin{prop}
Assume that $\hat{\mu}_t$ is indeed Gibbsian. If for each fixed $\eta\in\Omega_0$, there is a unique Gibbs measure associated with the Hamiltonians $(\H_\Lambda^{(t)}(\pika,\eta))_{\Lambda\Subset\Z^d}$ (which one could denote by $|\G_{\Omega_0}(\H^{(t)}(\pika,\eta))|=1$), then $\nu S(t)$ is Gibbsian.
\end{prop}
One can quickly notice a similarity with \textcolor{blue}{\texttt{Berghout's resut}}. \\

\textcolor{blue}{\texttt{Background about Dobrushin's uniqueness condition/high temperature Gibbs measures, if not done before.}}\\

The first result of interest is rather general, as it assumes nothing about the Gibbs measure of the initial configuration, other than its uniqueness w.r.t.~the associated interaction:

\begin{thm}
Let $\mu$ be \textcolor{blue}{either an infinite-~or high-temprature Gibbs measure}, that is, $\mu\in\G_{\Omega_0}(\Phi)$, where $\Phi$ is such that $|\G_{\Omega_0}(\Phi)|=1$. Then, $\mu S(t)$ is Gibbsian for each $t\geq 0$.
\end{thm}

Before stating the proof, we remark that $\H_\Lambda^t$ may be rewritten as
$$\H_\Lambda^{(t)}(\sigma_\Lambda,\eta_\Lambda) ~=~ \H_\Lambda(\sigma_\Lambda) - \sum_{x\in\Lambda}h_1^x(t)\sigma(x) - \sum_{x\in\Lambda}h_2^x(t)\eta(x) - \sum_{x\in\Lambda}h_{1,2}^x(t)\sigma(x)\eta(x), \quad \sigma,\eta\in\Omega_0,$$
where 
\begin{align*}
h_1^x(t) ~&=~ \frac{1}{4}\log\frac{p_t^x(+1,+1)p_t^x(+1,-1)}{p_t^x(-1,+1)p_t^x(-1,-1)}, \\
h_2^x(t) ~&=~ \frac{1}{4}\log\frac{p_t^x(+1,+1)p_t^x(-1,+1)}{p_t^x(+1,-1)p_t^x(-1,-1)} \\
h_{1,2}^x(t) ~&=~ \frac{1}{4}\log\frac{p_t^{x}(+1,+1)p_t^x(-1,-1)}{p_t^x{(+1,-1)}p_t^{x}(-1,+1)}.
\end{align*}
The assumption of independence of $p_t^x$ from $x$, allows us to simply write $h_1,h_2,h_{1,2}$. In fact, the precise expression by which $L_x$ is given also allows us to write those values out nicely, as can be seen in Equation (5.9) in \textcolor{red}{\texttt{[reference]}}, but is for our purposes unimportant.

\begin{proof}
In this case, \textcolor{blue}{we know} the joint distribution $\hat{\mu}_t$ of $(\omega_0,\omega_t)$ to be Gibbsian and consistent with the Hamiltonians $\H^{(t)}$, which can -- due to assumptions of independence of $L_x$ from $x$ -- be written out as
\begin{align*}
\H_\Lambda^{(t)}(\sigma_\Lambda,\eta_\Lambda) ~&=~ \H_\Lambda(\sigma_\Lambda) - \sum_{x\in\Lambda}\oklepaj{h_1(t)+h_{1,2}(t)\eta(x)}\sigma(x) - h_2(t)\sum_{x\in\Lambda}\eta(x).
\end{align*}
We wish to show, that for each fixed $\eta\in\Omega_0$, $|\G_{\Omega_0}(\H^{(t)}(\pika,\eta))|=1$, as the conclusion then follows from \textcolor{red}{\texttt{[reference of Proposition]}}. Indeed, fix arbitrary $\sigma\in\Omega_0$; we immediately notice, that the last term on the RHS is constant in $\sigma$, so we can consider an alternative collection of Hamiltonians $\tilde{\H}^{(t)}(\pika;\eta)$, given by
$$\tilde{\H}_\Lambda^{(t)}(\sigma_\Lambda;\eta) ~=~ \H_\Lambda(\sigma_\Lambda) - \sum_{x\in\Lambda}\oklepaj{h_1(t)+h_{1,2}(t)\eta(x)}\sigma(x),$$
which induces the same specification as $\H^{(t)}(\pika,\eta)$, and hence 
$$\G_{\Omega_0}(\tilde{\H}^{(t)}(\pika;\eta))=\G_{\Omega_0}(\H^{(t)}(\pika,\eta)).$$
\textcolor{blue}{Since $\tilde{\H}^{(t)}(\pika;\eta)$ differs from $\H$ only in the single site interaction, by \textcolor{red}{\texttt{[Lemma]}}, $\tilde{\H}^{(t)}(\pika;\eta)$ satisfies \textcolor{red}{\texttt{[Dobrushin]}} if and only if $\H$ satisfies \textcolor{red}{\texttt{[Dobrushin]}}}. $\H$ indeed satisfies it by assumption, from which it follows that $|\G_{\Omega_0}(\tilde{\H}^{(t)}(\pika;\eta))|=1$ and hence 
$$|\G_{\Omega_0}(\H^{(t)}(\pika,\eta))|=1.$$
As this works for any choice of $\eta$, \textcolor{red}{\texttt{[reference of Proposition]}} allows us to conclude.
\end{proof}

Another example the authors consider is one of the Ising model in the non-uniqueness regime, in particular of $\beta\gg\beta_c$. Here they make an additional assumption on the dynamics, which is that $\varepsilon=0$ in the definition of $L_x$, which translates on neither $+1$ nor $-1$ being favoured, and \textcolor{blue}{results in $h_1\equiv 0$, $h_2\equiv 0$} and $h_{1,2}(t)=:h_t=-\frac{1}{2}\log\tanh(\frac{t}{2})$.

\begin{thm}
Let $\mu_{\beta,h}$ denote some Gibbs measure of Ising model on $\Omega_0$ with $\beta\gg\beta_c$.
\begin{itemize}
	\item[(1)] There exists a $t_0=t_0(\beta,h)$, such that $\nu_{\beta,h}S(t)$ is Gibbsian for all $0\leq t\leq t_0$.
	\item[(2)] Moreover, if $h>0$, then there exists a $t_1=t_1(h)$, such that $\mu_{\beta,h}S(t)$ is Gibbsian for all $t\geq t_1$.
\end{itemize} 
\end{thm}

\begin{proof}
Recalling that the Hamiltonian of the Ising model is of form 
$$\H(\sigma)=-\beta\sum_{(x,y)}\sigma(x)\sigma(y)-h\sum_{x}\sigma(x),$$ 
\textcolor{blue}{slightly ignoring the finite volume and boundary condition business}, we obtain that $\H^{(t)}$ is of form
$$\H^{(t)}(\sigma,\eta) ~=~ -\beta\sum_{(x,y)}\sigma(x)\sigma(y)-h\sum_{x}\sigma(x) - h_t\sum_x \sigma(x)\eta(x).$$
\begin{itemize}
	\item[(1)] We want to exploit the fact that for small $t$, the ``dynamical field'' $h_t$ is large and -- intuitively speaking -- ``forces'' $\sigma$ in ``direction'' of $\eta$. In this spirit, we wish to rewrite the joint Hamiltonian as
\begin{align*}
\H^{(t)}(\sigma,\eta) ~&=~ \sqrt{h_t}\oklepaj{-\frac{\beta}{\sqrt{h_t}}\sum_{(x,y)}\sigma(x)\sigma(y)-\frac{h}{\sqrt{h_t}}\sum_{x}\sigma(x)-\sqrt{h_t}\sum_{x}\sigma(x)\eta(x)} \\
&=:~ \sqrt{h_t}\hat{\H}^{(t)}(\sigma,\eta).
\end{align*}
\textcolor{blue}{It is known from \textcolor{red}{\texttt{reference}}} that for fixed $\eta\in\Omega_0$ and for $0\leq t\leq t_0'$ small enough, $\eta$ is the unique ground state of $\hat{\H}^{(t)}(\pika,\eta)$, so for any $\lambda\geq\lambda_0$ large enough, $\lambda\hat{\H}^{(t)}$ satisfies \textcolor{red}{\texttt{[Dobrushin]}}. It follows that for $0\leq t\leq t_0$ such that $\sqrt{h_t}\geq \lambda_0$, $|\G_{\Omega_0}(\H^{(t)}(\pika,\eta))|$ for all $\eta\in\Omega_0$, allowing us to conclude.
	\item[(2)] We want to exploit the fact that $h_t$ is small for large field and is hence overpowered by $h$; this time around, we write
\begin{align*}
\H^{(t)}(\sigma,\eta) ~&=~ \beta\oklepaj{-\sqrt{\beta}\sum_{(x,y)}\sigma(x)\sigma(y)-\frac{h}{\beta}\sum_{x}\sigma(x)-\frac{h_t}{\sqrt{\beta}}\sum_{x}\sigma(x)\eta(x)} \\
&=:~ \sqrt{\beta}\overline{\H}^{(t)}(\sigma,\eta).
\end{align*}
\textcolor{blue}{For fixed $\eta\in\Omega_0$ and $t>t_1(h)$ large enough, $\frac{h}{|h|}$ is the unique ground state of $\overline{\H}^{(t)}(\pika,\eta)$.} Thus, again by \textcolor{red}{\texttt{[reference]}}, for $\beta$ large enough, $\sqrt{\beta}\overline{\H}^{(t)}(\pika,\eta)$ satisfies \textcolor{red}{\texttt{[Dobrushin]}}. \textcolor{purple}{Why is this enough? Is ``large enough'' independent of $\eta$? Or is there a uniform bound?}
\end{itemize}
\end{proof}

\begin{rem}
The same result can be obtained for $\varepsilon>0$, though in that case $t_i$'s also depend on the proportion $\frac{1-\varepsilon}{1+\varepsilon}$.
\end{rem}

% #################################################################################################

\subsection{Alternative approach: dynamics as a fuzzy map}

\textcolor{blue}{In the previous section, we have remarked on the similarity between \textcolor{red}{\texttt{[Proposition]}} and \textcolor{red}{\texttt{[Berghout's result]}}.} The aim of this section is firstly to argue that one can view the dynamics described in the previous section as a fuzzy model, and secondly to demonstrate that the proofs of those same results are very similar, despite the alternative approach. \\

Recall again that we assumed that the dynamics are i.i.d.\footnote{This is concluded by combining the assumption that $\P_\omega=\bigotimes_{x\in\Z^d}\P_{\omega(x)}$ and the assumption that $(L_x)_{x\in\Z^d}$ are independent of $x$.}, with no bias towards either $+1$ or $-1$ spins ($\varepsilon=0$). Considering the spin flip rates $c(x,\omega)$, the first assumption removes the dependence on the first coordinate and restricts the dependence in the second coordinate to only $\omega(x)$, while the second assumptions fully removes the dependence on the second coordinate. This yields that the dynamics are given by a collection of independent \textcolor{blue}{Poisson clocks} with rate $c>0$. \\

It is thus rather simple to describe probabilities of a certain value being assumed at a specific spin at any given time. Indeed, on has that, for $x\in\Z^d$ and $t>0$,
\begin{align*}
[\mu S(t)](\omega_t(x)=1) ~=~ \mu(\omega_0(x)=1)\P(\text{even flips of}~x) + \mu(\omega_0(x)=-1)\P(\text{odd flips of}~x),
\end{align*}
where $\P(\text{odd flips of}~x)$ of course corresponds to the probability of there being even number of arrivals of Poisson process with intensity $c$ until time $t$. Famously, the latter carries the probability of $\frac{1}{2}(1-e^{-2tc})$, yielding
$$[\mu S(t)](\omega_t(x)=1) ~=~ \mu(\omega_0(x)=1)\frac{1+e^{-2tc}}{2} + \mu(\omega_0(x)=-1)\frac{1-e^{-2tc}}{2}.$$

The assumptions of this particular model allow us to consider the alternative to considering the joint distribution of $(\omega_0,\omega_t)$, as was done before. In particular, we will rather focus on the joint distribution of $\omega_0$ and ``dynamics'' (that is, whether there was odd or even number of spins at any site) until time $t$. The advantage of this approach is, that while $\omega_t$ does indeed depend on $\omega_0$ (and their joint distribution is not necessarily Gibbsian), the information about the dynamics up until $t$ \textit{is} -- due to no bias via $\varepsilon=0$ -- independent of $\omega_0$. The joint distribution is thus a product measure and \textcolor{purple}{hence automatically Gibbsian} on $\Omega_0^2$. Moreover, it is also easy to see that such construction allows us to see each $\mu S(t)$ as a fuzzy Gibbs measure, allowing us to use tools from \textcolor{red}{\texttt{[second chapter]}}. \\

We first fix some value $\kappa\in[0,1]$ and define an $\Omega_0$-valued random variable $X\sim\mu$. Moreover, define another $\Omega_0$-valued random variable, $Y^\kappa$, whose law $\rho^\kappa$ is given as
$$\rho^\kappa ~=~ \oglati{(1-\kappa)\delta_{+1}-\kappa\delta_{\set{-1}}}^{\otimes\Z^d},$$
that is $(Y_x^\kappa)_{x\in\Z^d}$ are i.i.d.~with
$$Y_x^\kappa ~=~ \begin{cases}
1, \quad &\text{with probability}~1-\kappa,\\
-1, \quad &\text{with probability}~\kappa.	
\end{cases}$$
At last we define a random variable $Z^\kappa:=X\cdot Y^\kappa$, and denote its law by $\nu^\kappa$. Clearly, due to independence of $X$ and $Y^\kappa$, one has $(Z_i^\kappa)_{x\in\Z^d}$ i.i.d.~with 
$$Z_x^\kappa ~=~ \begin{cases}
X_i, \quad &\text{with probability}~1-\kappa,\\
-X_i, \quad &\text{with probability}~\kappa.
\end{cases}$$
It follows that arbitrary $x\in\Z^d$, one has
\begin{align*}
\nu^\kappa(Z_x^\kappa=1) ~&=~ \mu(X_x=1)\rho^\kappa(Y_x^\kappa=1) + \mu(X_x=-1)\rho^\kappa(Y_x^\kappa=-1) \\
&=~ \mu(X_x=1)(1-\kappa)+\mu(X_x=-1)\kappa.
\end{align*}
It is clear, that if define 
$$\kappa(t) ~=~ \frac{1-e^{-2ct}}{2}, \quad t\geq 0,$$
we obtain
$$\mu S(t) ~=~ \nu^{\kappa(t)}, \quad t\geq 0.$$
One the other hand, defining the map
\begin{align*}
\pi:\Omega_0\times\Omega_0&\ra\Omega_0 \\
(\omega^\c,\omega^\mathsf{d})&\mapsto\omega^\c\omega^\mathsf{d},
\end{align*}
we can clearly see that
$$\nu^{\kappa} ~=~ (\mu\otimes\rho^\kappa)\circ\pi^{-1}.$$
Clearly, such map is a surjection from $\Omega=\Omega_0^2$ to $\Sigma=\Omega_0$, so $\mu\otimes\rho^\kappa$ being Gibbsian, it yields that $\nu^\kappa$ is indeed a fuzzy Gibbs measure, so the same holds true for $\mu S(t)$, $t\geq 0$. \\

This allows us to utilize the \textcolor{blue}{fibre approach} on measures $\mu S(t)$. In this case, the particular fibres are very simple: for each $\sigma\in\Sigma$ and $\omega\in\Omega_0$, there a unique $\omega'\in\Omega_0$, such that $\omega\omega'=\sigma$. In fact, one precisely has that
$$\omega' ~=~ \oklepaj{\frac{\sigma(x)}{\omega(x)}}_{x\in\Z^d}.$$
Since individual sites can only assume values $+1$ and $-1$, we can utilize the fact that for $\alpha,\beta\in\set{-1,+1}$ one has $\frac{\alpha}{\beta}=\alpha\beta$, to write
$$\Omega_\sigma ~=~ \set{(\omega,\omega\sigma):\omega\in\Omega_0}.\footnote{Here, there should be no confusion between $\Omega_0=\set{-1,+1}^{\Z^d}$ and $\Omega_{\mathbf{0}}=\pi^{-1}(\mathbf{0})$, where $\mathbf{0}$ is the configuration with all sites equal to $0$.}$$ 

First we demonstrate, how we can prove \textcolor{red}{\texttt{[Theorem for unique $\mu$]}}, using the fibre approach.  Before starting, we remark that since $\mu\otimes\rho^\kappa$ is a product measure, its Hamiltonians $\H^\kappa=(\H_\Lambda^\kappa)_{\Lambda\Subset\Z^d}$ take the from
$$\H^\kappa(\omega^\c,\omega^\mathsf{d}) ~=~ \H(\omega^\c)-\sum_{x}\log\rho^\kappa(\omega^\mathsf{d}(x)),$$
where $\rho^\kappa(\omega^\mathsf{d}(x))$ equals $1-\kappa$ is $\omega^\mathsf{d}(x)=+1$ and $\kappa$ if $\omega^\mathsf{d}(x)=-1$.
In light of \textcolor{red}{\texttt{[Berghout's result]}} and the fact that $\set{\kappa(t):t\geq 0}\subset[0,1]$, we only need to prove the following:

\begin{prop}
For each $\kappa\in[0,1]$, we have $|\G_{\Omega_\sigma}(\H^\kappa)|=1$ for each $\sigma\in\Sigma$.
\end{prop}

\begin{proof}
Fixing some $\sigma\in\Sigma$, if one is to restrict possible values of $(\omega^\c,\omega^\mathsf{d})$ to $\Omega_\sigma$, we are left to consider $\H^\kappa(\omega,\omega\sigma)$, $\omega\in\Omega_0$, where
\begin{align*}
\H^\kappa(\omega,\omega\sigma) ~&=~ \H(\omega) - \sum_{x:\sigma(x)=1}\log\rho^\kappa(\omega(x)) - \sum_{x:\sigma(x)=-1}\log\rho^\kappa(-\omega(x)) \\
&=~ \H(\omega) - \sum_{x:\sigma(x)=1}\log\rho^\kappa(\omega(x)) - \sum_{x:\sigma(x)=-1}\log\!\oklepaj{1-\rho^\kappa(\omega(x))}.
\end{align*}
\textcolor{purple}{Similar as in the proof in the previous section, $(\omega\mapsto\H^\kappa(\omega,\omega\sigma))$ differs from $\H$ only by a single-site interaction}, yielding that one admits a unique Gibbs measure on $\Omega_0$ iff the other does. By assumption, $|\G_{\Omega_0}(\H)|=1$, so $|\G_{\Omega_0}(\H^\kappa(\pika,\pika\sigma))|=1$, regardless of $\sigma$. Given that $\Omega_\sigma=\set{(\omega,\omega\sigma):\omega\in\Omega}$, this is equivalent to $|\G_{\Omega_\sigma}(\H^\kappa)|=1$ for all $\sigma\in\Sigma$.
\end{proof}

In light of their proof of \textcolor{red}{\textcolor{red}{\texttt{Ising theorem}}}, in particular the fact that they argued via \textcolor{red}{\texttt{[Eq in Cor]}}, the sensible only way to obtain -- for sure -- the same $t_0$ and $t_1$, it to verify that we have Gibbsianity as soon as they have it. Due to the fact that they argued via \textcolor{red}{\texttt{[Eq in Cor]}}, it is sufficient to verify the following:
\begin{prop} Let $t\geq 0$ and $\sigma\in\Sigma$ be arbitrary. Then $\H^{(t)}(\pika,\sigma)$ and $\H^{\kappa(t)}(\pika,\pika\sigma)$ differ only by a single site interaction.
\end{prop}
\begin{proof}
In this particular case, one has
\begin{align*}
\H^{\kappa(t)}(\omega,\omega\sigma) ~&=~ -\beta\sum_{(x,y)}\sigma(x)\sigma(y) - h\sum_{x}\sigma(x) -\sum_{x}\log\rho^{\kappa(t)}(\omega(x)\sigma(x)), \omega\in\Omega.
\end{align*}
It follows that
\begin{align*}
\H^{\kappa(t)}(\omega,\omega\sigma) - \H^{(t)}(\omega,\sigma) ~&=~ h_t\sum_{x}\sigma(x)\eta(x) - \sum_{x}\log\rho^{\kappa(t)}(\omega(x)\sigma(x)),
\end{align*}
which indeed corresponds to a single-site interaction.
\end{proof}

This tells us that as soon as $|\G_{\Omega_0}(\H^{(t)}(\pika,\sigma)|=1$, one also has $|\G_{\Omega_0}^{\kappa(t)}(\omega,\omega\sigma)|=|\G_{\Omega_\sigma}(\H^{\kappa(t)})|=1$, so \textcolor{red}{\texttt{[Berghout's result]}} concludes the argument.

% #################################################################################################

\end{document}